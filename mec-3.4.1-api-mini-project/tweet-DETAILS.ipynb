{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# This file extracts tweets for a given period for a stock\n",
    "# It then gets details needed for the sentiment analysis\n",
    "# It saves these to a file tweetdetails.csv\n",
    "# It also gets corresponding data for the period for the same stock from a financial instituion such as Yahoo Finance / Nasdaq\n",
    "# and saves to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tweepy as twitter\n",
    "import os\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "from dotenv import load_dotenv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# get the API access keys from file .env\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "config = dotenv.dotenv_values(\".env\")   # config = {\"USER\": \"foo\", \"EMAIL\": \"foo@example.org\"}\n",
    "\n",
    "#print(config)\n",
    "\n",
    "# keys and tokens from the Twitter Dev Console\n",
    "consumer_key = config.get('consumer_key')\n",
    "consumer_secret = config.get('consumer_secret')\n",
    "access_token = config.get('access_token')\n",
    "access_token_secret = config.get('access_token_secret')\n",
    "bearer_token = config.get('bearer_token')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "setup access info for Twitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.client.Client object at 0x7fe74168b820>\n"
     ]
    }
   ],
   "source": [
    "auth = twitter.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "auth.access_token = access_token\n",
    "API = twitter.API(auth)\n",
    "\n",
    "client = twitter.Client(bearer_token)\n",
    "print(client)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# function to get tweet details\n",
    "def getTweetDetails():\n",
    "\n",
    "    # Replace with your own search query\n",
    "    query = 'TSLA'\n",
    "\n",
    "    # Replace with time period of your choice\n",
    "    start_time = '2022-04-11T00:00:00Z'\n",
    "\n",
    "    # Replace with time period of your choice\n",
    "    end_time = '2022-04-17T00:00:00Z'\n",
    "\n",
    "    tweets = client.search_recent_tweets(query=query, tweet_fields=['context_annotations', 'author_id',\n",
    "                                                                    'created_at', 'entities', 'public_metrics'],\n",
    "                                  user_fields=['id', 'username', 'public_metrics', 'verified'], expansions='author_id',\n",
    "                                  start_time=start_time,\n",
    "                                  end_time=end_time, max_results=100)\n",
    "\n",
    "    # tweets = API.search_all_tweets(query=query, tweet_fields=['context_annotations', 'created_at'],\n",
    "\n",
    "    # for each tweet get the user details as well\n",
    "    users = {u[\"id\"]: u for u in tweets.includes['users']}\n",
    "\n",
    "    # Creating DataFrame using pandas\n",
    "    db = pd.DataFrame(columns=['tweet_created_at',\n",
    "                               'tweet_user_id',\n",
    "                               'tweet_entities',\n",
    "                               'tweet_public_metrics',\n",
    "                               'tweet_text',\n",
    "                               'tweet_annotations',\n",
    "                               'tweet_user_id',\n",
    "                               'tweet_user_name',\n",
    "                               'tweet_user_metrics',\n",
    "                               'tweet_user_verified'])\n",
    "\n",
    "    # Counter to maintain Tweet Count\n",
    "    n = 1\n",
    "\n",
    "    for tweet in tweets.data:\n",
    "        tweet_created_at = tweet.created_at\n",
    "        tweet_user_id = tweet.author_id\n",
    "        tweet_entities = tweet.entities\n",
    "        tweet_public_metrics = tweet.public_metrics\n",
    "        tweet_text = tweet.text\n",
    "        if len(tweet.context_annotations) > 0:\n",
    "            tweet_annotations = tweet.context_annotations\n",
    "        #print(\"======================USER DETAILS===================================\")\n",
    "        if users[tweet.author_id]:\n",
    "            user = users[tweet.author_id]\n",
    "            tweet_user_id = user.id\n",
    "            tweet_user_name = user.name\n",
    "            tweet_user_metrics = user.public_metrics\n",
    "            tweet_user_verified =  user.verified\n",
    "        #print(\"================================NEXT TWEET====================================\")\n",
    "\n",
    "\n",
    "        # Here we are appending all the\n",
    "        # extracted information in the DataFrame\n",
    "        nth_tweet = [tweet_created_at,\n",
    "                     tweet_user_id,\n",
    "                     tweet_entities,\n",
    "                     tweet_public_metrics,\n",
    "                     tweet_text,\n",
    "                     tweet_annotations,\n",
    "                     tweet_user_id,\n",
    "                     tweet_user_name,\n",
    "                     tweet_user_metrics,\n",
    "                     tweet_user_verified]\n",
    "        db.loc[len(db)] = nth_tweet\n",
    "\n",
    "        # Function call to print tweet data on screen\n",
    "        n = n+1\n",
    "    filename = 'tweet_details.csv'\n",
    "\n",
    "    # we will save our database as a CSV file.\n",
    "    db.to_csv(filename)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# function to perform data extraction\n",
    "def getTweets(words, date_since, numtweet):\n",
    "\n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                   'description',\n",
    "                                   'location',\n",
    "                                   'following',\n",
    "                                   'followers',\n",
    "                                   'totaltweets',\n",
    "                                   'retweetcount',\n",
    "                                   'text',\n",
    "                                   'hashtags'])\n",
    "\n",
    "        # We are using .Cursor() to search\n",
    "        # through twitter for the required tweets.\n",
    "        # The number of tweets can be\n",
    "        # restricted using .items(number of tweets)\n",
    "        tweets = twitter.Cursor(API.search_tweets,\n",
    "                               words, lang=\"en\",\n",
    "                               since_id=date_since,\n",
    "                               tweet_mode='extended').items(numtweet)\n",
    "\n",
    "\n",
    "        # .Cursor() returns an iterable object. Each item in\n",
    "        # the iterator has various attributes\n",
    "        # that you can access to\n",
    "        # get information about each tweet\n",
    "        list_tweets = [tweet for tweet in tweets]\n",
    "\n",
    "        # Counter to maintain Tweet Count\n",
    "        i = 1\n",
    "\n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                location = tweet.user.location\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    "\n",
    "                # Retweets can be distinguished by\n",
    "                # a retweeted_status attribute,\n",
    "                # in case it is an invalid reference,\n",
    "                # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    "\n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description,\n",
    "                             location, following,\n",
    "                             followers, totaltweets,\n",
    "                             retweetcount, text, hashtext]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    "\n",
    "                # Function call to print tweet data on screen\n",
    "                #printtweetdata(i, ith_tweet)\n",
    "                i = i+1\n",
    "        filename = 'tweet__cursor_details.csv'\n",
    "\n",
    "        # we will save our database as a CSV file.\n",
    "        db.to_csv(filename)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def getYahooData():\n",
    "    df_yahoo = pdr.get_data_yahoo(\"TSLA\", start=\"2022-04-10\", end=\"2022-04-16\")\n",
    "    df_yahoo.tail()\n",
    "\n",
    "\n",
    "    data = yf.download(  # or pdr.get_data_yahoo(...\n",
    "            # tickers list or string as well\n",
    "            tickers = \"TSLA AAPL\",\n",
    "\n",
    "            # use \"period\" instead of start/end\n",
    "            # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "            # (optional, default is '1mo')\n",
    "            period = \"1mo\",\n",
    "\n",
    "            # fetch data by interval (including intraday if period < 60 days)\n",
    "            # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "            # (optional, default is '1d')\n",
    "            interval = \"1d\",\n",
    "\n",
    "            # group by ticker (to access via data['SPY'])\n",
    "            # (optional, default is 'column')\n",
    "            group_by = 'ticker',\n",
    "\n",
    "            # adjust all OHLC automatically\n",
    "            # (optional, default is False)\n",
    "            auto_adjust = True,\n",
    "\n",
    "            # download pre/post regular market hours data\n",
    "            # (optional, default is False)\n",
    "            prepost = True,\n",
    "\n",
    "            # use threads for mass downloading? (True/False/Integer)\n",
    "            # (optional, default is True)\n",
    "            threads = True,\n",
    "\n",
    "            # proxy URL scheme use use when downloading?\n",
    "            # (optional, default is None)\n",
    "            proxy = None\n",
    "        )\n",
    "\n",
    "    data.tail(10)\n",
    "    data.to_csv(\"yahoo_data/yfinance_data.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  2 of 2 completed\n",
      "Current directory = /Users/ravibyakod/WORK/Python/mec-mini-projects/mec-3.4.1-api-mini-project\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    getTweetDetails()\n",
    "    getYahooData()\n",
    "\n",
    "    # below is code for TESTING only using a tweepy.Cursor()\n",
    "    # no need to execute it but it works\n",
    "    aapl_tweets = API.search_tweets(q = '$AAPL')\n",
    "\n",
    "    # Enter Hashtag and initial date\n",
    "    # Enter Twitter Stock to search for\n",
    "    words = \"TSLA\"\n",
    "    # Enter Date since The Tweets are required in yyyy-mm--dd\"\n",
    "    date_since = \"2022-03-01\"\n",
    "\n",
    "    # number of tweets you want to extract in one run\n",
    "    numtweet = 100\n",
    "    getTweets(words, date_since, numtweet)\n",
    "\n",
    "\n",
    "    # SaveData\n",
    "    # what is current directory?\n",
    "    cwd = os.getcwd()\n",
    "    print(\"Current directory = \" + cwd)\n",
    "    # save it to a file in data_sets directory\n",
    "    file2 = open(r\"./datasets/twitter_api_search_query_results.txt\",\"w+\")\n",
    "    # write all tweets to the file\n",
    "    for tweet in aapl_tweets:\n",
    "        file2.write(\"{}\\n\".format(tweet))\n",
    "        #print(tweet['created_at'], tweet['id'], tweet['user']['id'], tweet['user']['name'], tweet['user']['followers_count'], tweet['text'])\n",
    "    # close the file handle\n",
    "    file2.close()\n",
    "\n",
    "# function to display data of each tweet\n",
    "def printtweetdata(n, ith_tweet):\n",
    "        print()\n",
    "        print(f\"Tweet {n}:\")\n",
    "        print(f\"Username:{ith_tweet[0]}\")\n",
    "        print(f\"Description:{ith_tweet[1]}\")\n",
    "        print(f\"Location:{ith_tweet[2]}\")\n",
    "        print(f\"Following Count:{ith_tweet[3]}\")\n",
    "        print(f\"Follower Count:{ith_tweet[4]}\")\n",
    "        print(f\"Total Tweets:{ith_tweet[5]}\")\n",
    "        print(f\"Retweet Count:{ith_tweet[6]}\")\n",
    "        print(f\"Tweet Text:{ith_tweet[7]}\")\n",
    "        print(f\"Hashtags Used:{ith_tweet[8]}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # calling main function\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}