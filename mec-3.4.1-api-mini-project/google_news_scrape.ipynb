{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# scrape stock data from google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random \n",
    "from collections import OrderedDict\n",
    "\n",
    "# List of header that contain User-Agent\n",
    "def list_header():\n",
    "    headers_list = [\n",
    "        # Firefox 24 Linux\n",
    "        {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:24.0) Gecko/20100101 Firefox/24.0',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        },\n",
    "        # Firefox Mac\n",
    "        {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "    ]\n",
    "    return headers_list\n",
    "\n",
    "def list_dict():\n",
    "    # Get headers list\n",
    "    headers_list = list_header()\n",
    "    # Create ordered dict from Headers above\n",
    "    ordered_headers_list = []\n",
    "    for headers in headers_list:\n",
    "        h = OrderedDict()\n",
    "        for header,value in headers.items():\n",
    "            h[header]=value\n",
    "        ordered_headers_list.append(h)\n",
    "    return ordered_headers_list\n",
    "\n",
    "def list_test():\n",
    "    headers_list = list_dict()\n",
    "    max = len(headers_list)\n",
    "    url = 'https://httpbin.org/headers'\n",
    "    for i in range(0,max):\n",
    "        #Pick a random browser headers\n",
    "        headers = random.choice(headers_list)\n",
    "        #Create a request session\n",
    "        r = requests.Session()\n",
    "        r.headers = headers\n",
    "        \n",
    "        response = r.get(url)\n",
    "        print(\"Request #%d\\nUser-Agent Sent:%s\\n\\nHeaders Recevied by HTTPBin:\"%(i,headers['User-Agent']))\n",
    "        print(response.json())\n",
    "        print(\"-------------------\")\n",
    "\n",
    "def random_header():\n",
    "    headers_list = list_dict()\n",
    "    headers = random.choice(headers_list)\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-03b24887a66a>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-03b24887a66a>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    print(Google search query ===>>> \" + url\")\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# INGESTION THROUGH WEB SCRAPING USING BEAUTIFULSOUP\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def ingest_google_news():\n",
    "    ticker_list = ['AAPL', 'GOOG', 'FB','NFLX', 'NVDA', 'ZM', 'ADBE', 'MSFT', 'TSLA']\n",
    "\n",
    "    sep = '.'\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    t_news = []\n",
    "    t_publisher = []\n",
    "    t_urls = []\n",
    "    t_dates = []\n",
    "    t_tickers = []\n",
    "\n",
    "    for t in ticker_list:\n",
    "        news = []\n",
    "        publisher = []\n",
    "        urls = []\n",
    "        dates = []\n",
    "        tickers = []\n",
    "\n",
    "        # cleaning ticker\n",
    "        ticker = t\n",
    "        t = t.split(sep, 1)[0]\n",
    "\n",
    "        # set header by random user agent \n",
    "        r = requests.Session()\n",
    "        headers = random_header()\n",
    "        r.headers = headers\n",
    "        # print(headers)\n",
    "\n",
    "        # set query for google\n",
    "        query = '{} stock news'.format(t)\n",
    "        url = f\"https://www.google.com/search?q={query}&tbm=nws&lr=lang_en&hl=en&sort=date&num=5\"\n",
    "        print(Google search query ===>>> \" + url\")\n",
    "        res = r.get(url, headers=headers)\n",
    "        soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        links = soup.select(\".dbsr a\")\n",
    "        for l in links:\n",
    "            tickers.append(t)\n",
    "            try:\n",
    "                url_w = l.get(\"href\")\n",
    "                print(\"follow on URL to scrape = \" + url_w)\n",
    "                urls.append(url_w)\n",
    "                dt = find_date(url_w)\n",
    "                dates.append(dt)\n",
    "\n",
    "                res = requests.get(url_w, headers=headers)\n",
    "                parsed_article = bs4.BeautifulSoup(res.text,'lxml')\n",
    "                paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "                article_text = \"\"\n",
    "                for p in paragraphs:\n",
    "                    article_text += p.text\n",
    "\n",
    "            except Exception as e:\n",
    "                article_text = ''\n",
    "\n",
    "            news.append(article_text)\n",
    "\n",
    "        sources = soup.select(\".XTjFC g-img\")\n",
    "        for s in sources:\n",
    "            publisher.append(s.next_sibling.lower())\n",
    "\n",
    "        t_urls += urls\n",
    "        t_news += news\n",
    "        t_publisher += publisher\n",
    "        t_dates += dates\n",
    "        t_tickers += tickers\n",
    "\n",
    "    df['ticker'] = t_tickers\n",
    "    df['links'] = t_urls\n",
    "    df['article_text'] = t_news\n",
    "    df['publisher'] = t_publisher\n",
    " #   df['created_at'] = t_dates\n",
    "\n",
    "    # import to csv\n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d%m%Y\")\n",
    "    df.to_csv(f'./datasets/google_news_{d1}.csv')\n",
    "\n",
    "    del news, publisher, urls, dates, tickers\n",
    "    del t_news, t_publisher, t_urls, t_dates, ticker\n",
    "\n",
    "\n",
    "ingest_google_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd-1",
   "language": "python",
   "name": "ucsd-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
