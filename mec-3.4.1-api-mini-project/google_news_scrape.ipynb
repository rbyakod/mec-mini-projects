{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# scrape stock data from google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import random \n",
    "from collections import OrderedDict\n",
    "\n",
    "# List of header that contain User-Agent\n",
    "def list_header():\n",
    "    headers_list = [\n",
    "        # Firefox 24 Linux\n",
    "        {\n",
    "            'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:24.0) Gecko/20100101 Firefox/24.0',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        },\n",
    "        # Firefox Mac\n",
    "        {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "    ]\n",
    "    return headers_list\n",
    "\n",
    "def list_dict():\n",
    "    # Get headers list\n",
    "    headers_list = list_header()\n",
    "    # Create ordered dict from Headers above\n",
    "    ordered_headers_list = []\n",
    "    for headers in headers_list:\n",
    "        h = OrderedDict()\n",
    "        for header,value in headers.items():\n",
    "            h[header]=value\n",
    "        ordered_headers_list.append(h)\n",
    "    return ordered_headers_list\n",
    "\n",
    "def list_test():\n",
    "    headers_list = list_dict()\n",
    "    max = len(headers_list)\n",
    "    url = 'https://httpbin.org/headers'\n",
    "    for i in range(0,max):\n",
    "        #Pick a random browser headers\n",
    "        headers = random.choice(headers_list)\n",
    "        #Create a request session\n",
    "        r = requests.Session()\n",
    "        r.headers = headers\n",
    "        \n",
    "        response = r.get(url)\n",
    "        print(\"Request #%d\\nUser-Agent Sent:%s\\n\\nHeaders Recevied by HTTPBin:\"%(i,headers['User-Agent']))\n",
    "        print(response.json())\n",
    "        print(\"-------------------\")\n",
    "\n",
    "def random_header():\n",
    "    headers_list = list_dict()\n",
    "    headers = random.choice(headers_list)\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR LOOPING THE COMPANY INTO WIKIPEDIA INGESTION\n",
    "def ingest_wiki():\n",
    "    # ticker example\n",
    "    ticker_list = ['AAPL.O', 'LVS', 'JPM', 'XOM', '005930.KS']\n",
    "    ticker_name = ['APPLE INC', 'LAS VEGAS SANDS CORP', 'JPMORGAN CHASE & CO', 'EXXON MOBIL CORP', 'SAMSUNG ELECTRONICS']\n",
    "\n",
    "    news = []\n",
    "    links = []\n",
    "    tickers = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for t in ticker_name:\n",
    "        # get company description from ingest_wikipedia function\n",
    "        doc = ingest_wikipedia(t)\n",
    "        news.append(doc[0])\n",
    "        links.append(doc[1])\n",
    "    \n",
    "    # store to csv\n",
    "    df['ticker'] = ticker_list\n",
    "    df['ticker_name'] = ticker_name\n",
    "    df['link'] = links\n",
    "    df['news'] = news\n",
    "    df.to_csv('data_news/wikipedia-tickers.csv')\n",
    "\n",
    "    del news, links, tickers\n",
    "    \n",
    "# FUNCTION FOR WIKIPEDIA INGESTION\n",
    "def ingest_wikipedia(ticker_name):\n",
    "    # set header\n",
    "    headers = random_header()\n",
    "\n",
    "    # query for wikipedia and tickername\n",
    "    url = f\"https://www.google.com/search?q=wikipedia {ticker_name.lower()} company&lr=lang_en&hl=en\"\n",
    "    res = requests.get(url, headers=headers)\n",
    "    # status = res.raise_for_status()\n",
    "\n",
    "    url_w = []\n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\".yuRUbf a\")\n",
    "    \n",
    "    for link in links[:5]:\n",
    "        url_w.append(link.get(\"href\"))\n",
    "    \n",
    "    print(url_w[0])\n",
    "\n",
    "    # open wikipedia site and get company description\n",
    "    try:\n",
    "        scrapped_data = urllib.request.urlopen(url_w[0])\n",
    "\n",
    "        article = scrapped_data.read()\n",
    "        parsed_article = bs4.BeautifulSoup(article,'lxml')\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "        article_text = \"\"\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_text += p.text\n",
    "            \n",
    "        link = url_w[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        article_text = ''\n",
    "        link = ''\n",
    "\n",
    "    # return company description and wikipedia link\n",
    "    return article_text, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pulse2.com/xom-stock-69-target-from-bmo-capital/\n",
      "https://www.fool.com/investing/2021/07/19/why-energy-stocks-like-exxonmobil-phillips-66-and/\n",
      "https://finance.yahoo.com/news/exxon-mobil-xom-stock-sinks-214509570.html\n",
      "https://www.marketwatch.com/story/exxon-mobil-corp-stock-rises-wednesday-outperforms-market-01626903084-c6dd31484cb5\n",
      "https://investorplace.com/2021/07/despite-the-activists-exxonmobil-will-be-just-fine/\n"
     ]
    }
   ],
   "source": [
    "# INGESTION THROUGH WEB SCRAPING USING BEAUTIFULSOUP\n",
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "def ingest_google_news():\n",
    "    ticker_list = ['AAPL.O', 'LVS', 'COTY.K','JPM', 'XOM', '005930.KS']\n",
    "\n",
    "    sep = '.'\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    t_news = []\n",
    "    t_publisher = []\n",
    "    t_urls = []\n",
    "    t_dates = []\n",
    "    t_tickers = []\n",
    "\n",
    "    for t in ticker_list:\n",
    "        news = []\n",
    "        publisher = []\n",
    "        urls = []\n",
    "        dates = []\n",
    "        tickers = []\n",
    "\n",
    "        # cleaning ticker\n",
    "        ticker = t\n",
    "        t = t.split(sep, 1)[0]\n",
    "\n",
    "        # set header by random user agent \n",
    "        r = requests.Session()\n",
    "        headers = random_header()\n",
    "        r.headers = headers\n",
    "        # print(headers)\n",
    "\n",
    "        # set query for google\n",
    "        query = '{} stock news'.format(t)\n",
    "        url = f\"https://www.google.com/search?q={query}&tbm=nws&lr=lang_en&hl=en&sort=date&num=5\"\n",
    "        res = r.get(url, headers=headers)\n",
    "        soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        links = soup.select(\".dbsr a\")\n",
    "        for l in links:\n",
    "            tickers.append(t)\n",
    "            try:\n",
    "                url_w = l.get(\"href\")\n",
    "                print(url_w)\n",
    "                urls.append(url_w)\n",
    "                dt = find_date(url_w)\n",
    "                dates.append(dt)\n",
    "\n",
    "                res = requests.get(url_w, headers=headers)\n",
    "                parsed_article = bs4.BeautifulSoup(res.text,'lxml')\n",
    "                paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "                article_text = \"\"\n",
    "                for p in paragraphs:\n",
    "                    article_text += p.text\n",
    "\n",
    "            except Exception as e:\n",
    "                article_text = ''\n",
    "\n",
    "            news.append(article_text)\n",
    "\n",
    "        sources = soup.select(\".XTjFC g-img\")\n",
    "        for s in sources:\n",
    "            publisher.append(s.next_sibling.lower())\n",
    "\n",
    "        t_urls += urls\n",
    "        t_news += news\n",
    "        t_publisher += publisher\n",
    "        t_dates += dates\n",
    "        t_tickers += tickers\n",
    "\n",
    "    df['ticker'] = t_tickers\n",
    "    df['links'] = t_urls\n",
    "    df['article_text'] = t_news\n",
    "    df['publisher'] = t_publisher\n",
    " #   df['created_at'] = t_dates\n",
    "\n",
    "    # import to csv\n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%d%m%Y\")\n",
    "    df.to_csv(f'google_news_{d1}.csv')\n",
    "\n",
    "    del news, publisher, urls, dates, tickers\n",
    "    del t_news, t_publisher, t_urls, t_dates, ticker\n",
    "\n",
    "\n",
    "ingest_google_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd-1",
   "language": "python",
   "name": "ucsd-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
